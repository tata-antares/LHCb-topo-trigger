{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHUNKSIZE = 5000\n",
    "def concat_files(folder, names, outfile):\n",
    "    head = True\n",
    "    mode = 'w'\n",
    "    for name in names:\n",
    "        pd_iterator = pandas.read_csv(os.path.join(folder, name), sep='\\t', iterator=True, chunksize=CHUNKSIZE)\n",
    "        for pd in pd_iterator:\n",
    "            pd.to_csv(outfile, header=head, index=False, sep='\\t', mode=mode)\n",
    "            head = False\n",
    "            mode = 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rep.utils import train_test_split_group\n",
    "def prepare_data(signal_data, bck_data, group_column, random_state=13):\n",
    "    ds_train_signal, ds_test_signal = train_test_split_group(signal_data[group_column], signal_data, \n",
    "                                                             train_size=0.5, test_size=0.5, \n",
    "                                                             random_state=random_state)\n",
    "                                                             \n",
    "\n",
    "    ds_train_bck, ds_test_bck = train_test_split_group(bck_data[group_column], bck_data, \n",
    "                                                       train_size=0.5, test_size=0.5, \n",
    "                                                       random_state=random_state)\n",
    "\n",
    "    return ds_train_signal, ds_train_bck, ds_test_signal, ds_test_bck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statistic_length(data):\n",
    "    return {'Events': len(numpy.unique(data['unique'])), 'SVR': len(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def statistic_length(data):\n",
    "#     events_ids = data['mode'] * 100000000 + data['event_number']\n",
    "#     return {'Events': len(numpy.unique(events_ids)), 'SVR': len(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def result_statistic(models, modes, data, thresholds, rates, total_events):\n",
    "    # dict((name, rate): dict(mode: eff))\n",
    "    modes_eff = defaultdict(OrderedDict)\n",
    "    statistic = defaultdict(list)\n",
    "    for name, cl in models.items():\n",
    "        for mode in modes:\n",
    "            sig_mode = data[data['mode'] == mode]\n",
    "            if len(sig_mode) <= 0:\n",
    "                continue\n",
    "            statistic['mode'].append(mode)\n",
    "            statistic['classifier'].append(name)\n",
    "            latex_name = '$' + Samples[str(mode)]['root'].replace(\"#\", \"\\\\\") + '$'\n",
    "            statistic['name'].append(latex_name)\n",
    "            sig_prediction = cl.predict_proba(sig_mode)[:, 1]\n",
    "            sig_event_prediction = voting_for_event_svr(sig_mode['event_number'], sig_prediction)\n",
    "            for rate in rates:\n",
    "                # important: greater, not >=\n",
    "                thr = thresholds[name][rate]\n",
    "                exist_sig = numpy.sum(sig_event_prediction > thr)\n",
    "                eff = exist_sig * 1. / total_events[mode]\n",
    "                statistic[rate].append(eff * 100.)\n",
    "                modes_eff[(name, rate)][latex_name] = eff\n",
    "    return modes_eff, statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empty_events = dict()\n",
    "with open('generate_hlt1.log', 'r') as file:\n",
    "    for line in file:\n",
    "        if 'Mode' in line:\n",
    "            mode = line.strip().split(':')[-1]\n",
    "        elif 'No sv' in line:\n",
    "            empty = int(line.strip().split(':')[-1])\n",
    "        else:\n",
    "            empty_events[int(mode)] = empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_eff_dict(eff):\n",
    "    for key, val in eff.items():\n",
    "        print key\n",
    "        for k, v in val.items():\n",
    "            print k, ':', v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "def calculate_thresholds(test_bck, y_pred_bck, total_bck_events, rates, id_column='event_number'):\n",
    "    required_size = dict([(rate, int(rate * total_bck_events / 1e6)) for rate in rates])\n",
    "    bck_event_prediction = voting_for_event_svr(test_bck[id_column], y_pred_bck[:, 1])\n",
    "    thresholds = dict()\n",
    "    result = dict()\n",
    "    for rate, req_size in required_size.items():\n",
    "        threshold = numpy.percentile(bck_event_prediction, 100. * (1 - req_size * 1. / len(bck_event_prediction)))\n",
    "        thresholds[rate] = threshold\n",
    "        exist_bck = numpy.sum(bck_event_prediction > threshold)\n",
    "        result[rate] = (threshold, exist_bck, 1. * exist_bck / total_bck_events)\n",
    "    return thresholds, result\n",
    "\n",
    "def voting_for_event_svr(ids, prediction):\n",
    "    df = pandas.DataFrame({'prediction': numpy.array(prediction), 'id': numpy.array(ids)})\n",
    "    return numpy.array(df.groupby('id')['prediction'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "def plot_roc_events(estimator, sig_body, bck_body, label='', normed_channels=True, not_passed_events_sig=0, \n",
    "                    not_passed_events_bck=0):\n",
    "    test_all = pandas.concat([sig_body, bck_body])\n",
    "    pred_all = estimator.predict_proba(test_all)[:, 1]\n",
    "    sig_ = voting_for_event_svr(sig_body['unique'].values, pred_all[:len(sig_body)])\n",
    "    sig_mode = voting_for_event_svr(sig_body['unique'].values, numpy.array(sig_body['mode']))\n",
    "    sig_ = numpy.concatenate([sig_, numpy.zeros(not_passed_events_sig)])\n",
    "\n",
    "    sample_weight = numpy.ones(len(sig_))\n",
    "    if normed_channels:\n",
    "        for val in numpy.unique(sig_mode):\n",
    "            sample_weight[sig_mode == val] /= 1. * len(sample_weight[sig_mode == val]) \n",
    "        \n",
    "    bck_ = voting_for_event_svr(bck_body['unique'].values, pred_all[len(sig_body):])\n",
    "    bck_ = numpy.concatenate([bck_, numpy.zeros(not_passed_events_bck)])\n",
    "    fpr, tpr, _ = roc_curve([1] * len(sig_) + [0] * len(bck_), numpy.concatenate([sig_, bck_]),\n",
    "                           sample_weight=numpy.concatenate([sample_weight, numpy.ones(len(bck_))]))\n",
    "    print label, 'AUC:', roc_auc_score([1] * len(sig_) + [0] * len(bck_), numpy.concatenate([sig_, bck_]),\n",
    "                                       sample_weight=numpy.concatenate([sample_weight, numpy.ones(len(bck_))]))\n",
    "    return fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_n_events_pass_threshold(event_number, predictions, threshold):\n",
    "    return (numpy.bincount(event_number, weights=predictions > threshold) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_topo_metric(test_bck, test_sig_mode, total_bck_events, \n",
    "                         total_sig_mode_events, rate, id_column='event_number'):\n",
    "    def topo_metric(y_true, y_pred, sample_weight=None):\n",
    "        threshold, _ = calculate_thresholds(test_bck, y_pred[y_true == 0], \n",
    "                                         total_bck_events, [rate], id_column=id_column)\n",
    "        threshold = threshold[rate]\n",
    "        exist_sig = compute_n_events_pass_threshold(numpy.array(test_sig_mode[id_column]), y_pred[y_true==1, 1], threshold)\n",
    "        return  exist_sig * 1. / total_sig_mode_events\n",
    "    return topo_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_eff_for_mode(test, prediction, total_events, threshold, id_column='event_number'):\n",
    "    event_prediction = voting_for_event_svr(numpy.array(test[id_column]), prediction[:, 1])\n",
    "    exist = numpy.sum(event_prediction >= threshold)\n",
    "    return exist * 1. / total_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run scripts/HltSamples.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_svr(data, estimator, count=1):\n",
    "    probs = estimator.predict_proba(data)[:, 1]\n",
    "    train_prob = data.copy()\n",
    "    train_prob['prediction'] = probs\n",
    "\n",
    "    good_events = train_prob[train_prob['signal'] == 0].copy()\n",
    "    add_events = []\n",
    "    for num, group in train_prob[train_prob['signal'] == 1].groupby('unique'):\n",
    "        index = numpy.argsort(group['prediction'].values)[::-1]\n",
    "        add_events.append(group.iloc[index[:count], :])\n",
    "    good_events = pandas.concat([good_events] + add_events)\n",
    "    print len(good_events)\n",
    "    return good_events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
